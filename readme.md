# Topic Mapping & Specificity Pipeline

This repository provides a unified pipeline to:

1. **Generate embeddings** for multiple text collections using different embedding methods (Doc2Vec, Sentence-BERT, RepLLaMA).
2. **Train topic models** (LDA or BERTopic) with 50 topics on each collection.
3. **Map** embedding features to topic-document distributions.
4. **Calculate specificity** scores for each topic based on embedding-to-topic mappings.
5. **Visualize** mapped vs. unmapped topics sorted by specificity.

---

## ğŸ—‚ï¸ Project Structure

```plaintext
project_root/
â”œâ”€â”€ Embeddings/
â”‚   â”œâ”€â”€ embed_doc2vec.py
â”‚   â”œâ”€â”€ embed_sbert.py
â”‚   â”œâ”€â”€ embed_repllama.py
â”‚
â”œâ”€â”€ TopicModels/
â”‚   â”œâ”€â”€ lda_train.py
â”‚   â”œâ”€â”€ bertopic_train.py
â”‚
â”œâ”€â”€ Mapping/
â”‚   â”œâ”€â”€ map_repllama_lda.py
â”‚   â”œâ”€â”€ map_repllama_bertopic.py
â”‚
â”œâ”€â”€ Visualisation/
â”‚   â””â”€â”€ visualise_mapping.py
â”‚
â”œâ”€â”€ embed_utils.py
â”œâ”€â”€ preprocess.py
â”œâ”€â”€ run_pipeline.py
â””â”€â”€ README.md
```

**Processed Data** go to:

```
Processedwiki/
Processed20ng/
Processedwsj/
```

Each contains:

* `<collection>_<method>_corpus_embeddings.pkl`
* `<collection>_<method>_projected_features.pkl`

**Results** go to:

```
Results/
â”œâ”€â”€ LDA/
â”‚   â”œâ”€â”€ wiki_dictionary.dict
â”‚   â”œâ”€â”€ wiki_corpus.pkl
â”‚   â”œâ”€â”€ wiki_lda50.model
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ BERTopic/
â”‚   â”œâ”€â”€ wiki_bertopic_model/   
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ wiki_repllama_lda_mapping.pkl
â”œâ”€â”€ wiki_repllama_bertopic_mapping.pkl
â”œâ”€â”€ ...
â”œâ”€â”€ lda_repllama.pdf
â””â”€â”€ bertopic_repllama.pdf
```

---

## âš™ï¸ Installation & Dependencies

1. **Clone repository**:

   ```bash
   git clone <repo_url>
   cd project_root
   ```

2. **Create Python environment** (recommended):

   ```bash
   python3 -m venv venv
   source venv/bin/activate
   ```

3. **Install dependencies**:

   ```bash
   pip install -r requirements.txt
   ```

> **requirements.txt** should include:
>
> * `gensim`
> * `sentence-transformers`
> * `bertopic`
> * `scikit-learn`
> * `spacy` (+ `en_core_web_sm` model)
> * `torch`
> * `numpy`
> * `matplotlib`

---

## ğŸš€ Usage

Run the full pipeline with a single command. Specify the embedding method (`doc2vec`, `sbert`, or `repllama`) and the topic model (`lda` or `bertopic`):

```bash
python run_pipeline.py --lang_model <doc2vec|sbert|repllama> --topic_model <lda|bertopic>
```

**Examples:**

* Doc2Vec + LDA:

  ```bash
  python run_pipeline.py --lang_model doc2vec --topic_model lda
  ```

* SBERT + LDA:

  ```bash
  python run_pipeline.py --lang_model sbert --topic_model lda
  ```

* RepLLaMA + BERTopic:

  ```bash
  python run_pipeline.py --lang_model repllama --topic_model bertopic
  ```

This will:

1. Embed each collection (or skip if cached).
2. Train the selected topic model.
3. Map RepLLaMA features to topics (if `--lang_model repllama`).
4. Generate a scatter plot saved under `Results/`.

---

## ğŸ”¬ Experiments Explained

We perform a controlled comparison across **three text collections**:

* **WSJ**: Wall Street Journal news articles.
* **20NG**: 20 Newsgroups posts.
* **Wiki**: Wikipedia article snippets.

**Embedding Methods**:

* **Doc2Vec**: Unsupervised doc embeddings via gensim.
* **SBERT**: Pretrained Sentence-BERT embeddings.
* **RepLLaMA**: LLM-based embeddings from a LoRA-fine-tuned Llama-2 model.

**Topic Models**:

* **LDA** (gensim): Classic probabilistic topic model.
* **BERTopic**: Transformer-based topic extraction with embeddings + clustering.

**Mapping**:

* For **RepLLaMA**, we map each embedding feature to the most aligned topic via a greedy algorithm, saving `*_repllama_lda_mapping.pkl` or `*_repllama_bertopic_mapping.pkl`.

**Specificity Scores**:

* Calculate per-topic scores by thresholding document-topic probabilities (GMM/median/percentile) and measuring how strongly topics concentrate on a subset of docs.

**Visualization**:

* Scatter plots of specificity: mapped features (alpha=1.0) vs. unmapped (alpha=0.3), sorted by score, across all collections.

This setup allows us to compare how different embedding and topic modeling choices affect topic specificity and feature alignment.

---

## ğŸ”„ Extending the Pipeline

* **Add new embeddings**: create `embed_newmethod.py` using `embed_utils.py` + `preprocess.py`.
* **Add more topic models**: implement `TopicModels/newmodel_train.py` and mapping in `Mapping/`.
* **Customize metrics**: modify `topic_specificity.py` or `topic_specificity_bertopic.py`.

---

## ğŸ“„ License

[MIT License](LICENSE)
